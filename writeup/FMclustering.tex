\documentclass{statsoc}
\usepackage{natbib}
\usepackage[margin=0.4in]{geometry}
\usepackage{bm}
\usepackage{rotating}
\setlength{\parindent}{0pt}
\linespread{1.2}
\title[Time series clustering based on Factor Modeling]{Time Series Clustering based on Factor Modeling}
\author[Xiang Zhu{\it et al.}]{Xiang Zhu}
\address{The University of Chicago, Chicago, USA.}
\email{xiangzhu@uchicago.edu}
\begin{document}
\begin{abstract}
[\sf TBA]
\end{abstract}
\keywords{[\sf TBA]}
\section{Introduction}
[{\sf Factor modelling}:] In this paper we deal the clustering of their variates based on the factor modelling of the multiple time series. Factor modelling multivariate time series arises from a dimension-reduction viewpoint. Differently from the factor analysis for independent samples, we search for the factors which drive the serial dependence of the original time series. The factor model used in this paper can be traced back at least to that of \cite{pb_fm87}. We decompose a multivariate time series into two parts: a dynamic part driven by a lower-dimensional factor time series and a static part which is a vector white noise.\\
$[${\sf High-dimensional time series}:$]$ High-dimensional time series data are often encountered nowadays in many fields including  finance, economics, environmental and medical studies. For instance, understanding the dynamics of the returns of large number of assets is the key for asset pricing, portfolio allocation and risk management. Environmental time series are often of high-dimension due to a large number of indices monitored across many different locations.\\
$[${\sf Clustering}:$]$ Clustering is an unsupervised learning procedure to identify hidden structure in the data set without ground truth labels. Data are organized into homogeneous classes where the with-in-group similarity and the between-group dissimilarity are maximized. When the data are time series, clustering would be useful to detect a few representative temporal patterns, quantify the affinity through the time or make forecasting. There are several features of time series which increase the difficulty of clustering. First of all, time series data are dynamic in essence and there are natural orders inside, so the temporal evolution should be considered in the analysis of similarities between series. In addition, time series databases are usually formed by large amounts of records and most of the standard clustering algorithms do not work efficiently on high-dimensional data. Time series clustering has become a very active research area nowadays. Refer the detailed and extensive review on clustering of time series by \cite{liao}.\\
$[${\sf Structure:}$]$ The rest of the paper is organized as follows. Section 2 first summarizes the factor modelling method for multiple time series \citep{ly_fm12} and then introduce the clustering approach based on the estimated factor loading matrix. Section 3 presents the performance comparisons between our method and several other existing methods both simulated dataset and classic  test datasets for time series clustering. Two real data applications are discussed in section 4
\section{Method}
\subsection{Clustering based on factor model}
Suppose that there is some linear dynamic structure in the original $p-$variate time series $\{{\bf y}_t\}_{t\in[T]}$, $[T]=\{1,\ldots,T\}$. Conceptually, ${\bf y}_t$ can be decomposed into two parts, a static part (i.e. white noise) and a dynamic part driven by a low-dimensional process
\begin{equation}\label{FM}
{\bf y}_t={\bf A}{\bf x}_t+{\bm\epsilon}_t,
\end{equation}
where ${\bf x}_t$ is an $r\times 1$ latent process with (unknown) $r\leq p$, $\bf A$ is a $p\times r$ unknown constant matrix, and ${\bm \epsilon}_t\sim{\sf WN}({\bm\mu}_\epsilon,{\bf\Sigma}_\epsilon)$ is a $p-$dimensional white-noise process. When $r$ is much smaller than $p$, the serial dependence of ${\bf y}_t$ is described by a lower-dimensional process ${\bf x}_t$, which is called a factor process. An effective dimension reduction is thus achieved in this way. The factor model (\ref{FM}) above can be traced back at least to \citet{pb_fm87}. For each time $t\in[T]$ and each variate $i\in[p]$,
\[y_{ti}=a_{i1}x_{t1}+a_{i2}x_{t2}+\ldots+a_{ir}x_{tr},\]
where $y_{ti}$ and $x_{tj}$ are the $i$th and $j$th components of ${\bf y}_t$ and ${\bf x}_t$ respectively. Factor modelling of $\{{\bf y}_t\}_{t\in[T]}$ implies that the temporal dynamics of its $i$th variate is totally determined by the $i$th row $A_i$ of $\bf A$. In this work, the similarity between time series in different variates is calibrated by their hidden linear dynamic structures. More precisely, variates $m$ and $n$ in $\{{\bf y}_t\}$ are similar if the factor loadings $A_m$ and $A_n$ are similar. Hence, clustering of $p$ time series is simplified as clustering $p$ row vectors in the loading matrix $\bf A$. The key for the clustering $p-$dimensional time series ${\bf y}_t$ is to estimate the loading matrix $\bf A$. In a sense, the loading matrix conveys all useful information about the stochastic structure of the process since $[${\sf WHAT?}$]$. Then a measure of structural diversity between two variates $i$ and $j$ in multiple time series ${\bf y}_t$ can be obtain by comparing the respective loadings $A_i$ and $A_j$ and a metric of similarity between these two variates are defined as
\[d(i,j)=\sqrt{\sum_{k=1}^r(a_{ik}-a_{jk})^2}.\]
$[${\sf Check the properties of this metric!}$]$. Since the white noise exhibits no serial correlations, the decomposition in the factor modelling is unique in the sense that both the number of factors (the dimension of the factor process) and the factor loading space are identifiable. (\textsf{Rotation difference?}) Thus, $d(i,j)$ always exists and satisfies the classical properties of a distance, i.e. non-negativity, symmetry and triangular inequality.
\subsection{Factor modelling for time series}
\citet{ly_fm12},\citet{lyb_fm11}.\\
To make the factor model (\ref{FM}) identifiable, several assumptions are required.
\begin{itemize}
\item[1] No linear combinations of ${\bf x}_t$ are white noise: otherwise such components can be absorbed into ${\bm \epsilon}_t$.
\item[2] The rank of $\bf A$ is $r$: otherwise the model can be expressed equivalently in terms of a lower-dimensional factor.
\item[3] Columns of $\bf A:=({\bf a}_1,\ldots,{\bf a}_r)$ are orthonormal: even though $\bf A$ and ${\bf x}_t$ are not uniquely determined but the {\it factor loading space} ${\cal M}({\bf A})$, that is, the $r-$dimensional linear space spanned by the columns of $\bf A$, is uniquely defined.
\end{itemize}
The key for the inference for factor model is to determine the number of factors $r$ and to estimate the $p\times r$ factor loading matrix $\bf A$, or more precisely the factor loading space ${\cal M}({\bf A})$.
This paper assumes that ${\bf x}_t$ is weakly stationary and the future white-noise components are uncorrelated with the factors up to the present. (Note that ${\bf x}_t$ and ${\bm\epsilon}_s$ are assumed to be uncorrelated for any $t$ and $s$ in most factor modelling literature).Put
\[{\bm\Sigma}_y(k)={\sf Cov}({\bf y}_{t+k},{\bf y}_t),~{\bm\Sigma}_x(k)={\sf Cov}({\bf x}_{t+k},{\bf x}_t),~{\bm \Sigma}_{x\epsilon}={\sf Cov}({\bf x}_{t+k},{\bm\epsilon}_{t}).\]
For prescribed integer $k_0\geq 1$, define
\[{\bf M}=\sum_{k=1}^{k_0}{\bm\Sigma}_y(k){\bm\Sigma}^{\sf T}_y(k).\]
Consider $p\times(p-r)$ matrix ${\bf B}=({\bf b}_1,\ldots,{\bf b}_{p-r})$ associated with the factor loading matrix $\bf A$ for which $({\bf A},{\bf B})$ forms a $p\times p$ orthogonal matrix: ${\bf B}^{\sf T}{\bf A}={\bf 0}$ and ${\bf B}^{\sf T}{\bf B}={\bf I}_{p-r}$.Simple algebra yields that ${\bf MB=0}$, implying that the columns of $\bf B$ are the eigenvectors of $\bf M$ corresponding to zero-eigenvalues. Thus, {\em the factor loading space ${\cal M}({\bf A})$ is spanned by the eigenvectors of $\bf M$ corresponding to its non-zero eigenvalues and the number of the nonzero eigenvalues is $r$.}The sum-quantity $\bf M$ accumulates the information from different time lags and this is useful when the sample size $n$ is small. Nonnegative definite matrix ${\bf \Sigma}_y(k){\bf\Sigma}^{\sf T}_y(k)$ is used to avoid cancellation of information from different time lags. Small values of $k_0$ are favoured, since the autocorrelation is often at its strongest at the small time lags. Estimation for ${\bm\Sigma}_y(k)$ with larger $k$ is less accurate.To estimate $r$ and ${\cal M}({\bf A})$, just perform an eigenanalysis on 
\[{\hat{\bf M}}=\sum_{k=1}^{k_0}{\hat{\bm\Sigma}}_y(k){\hat{\bm\Sigma}}^{\sf T}_y(k),\]
where ${\hat{\bm\Sigma}}_y(k)$ denotes the sample covariance matrix of ${\bf y}_t$ at time lag $k$
\[{\hat{\bm\Sigma}}_y(k)=\frac{1}{T-k}\sum_{t=1}^{T-k}({\bf y}_{t+k}-{\bar{\bf y}})\cdot({\bf y}_{t}-{\bar{\bf y}})^{\sf T},~~{\bar{\bf y}}=\frac{1}{T}\sum_{t=1}^T{\bf y}_t.\] 
The estimation of factor model is run as follows.
\begin{enumerate}
\item A ratio-based estimator for the number of factors $r$:
\[{\hat r}=\arg\min_{1\leq i\leq R}\frac{{\hat\lambda}_{i+1}}{{\hat\lambda}_i},\]
where ${\hat\lambda}_1\geq\ldots{\hat\lambda}_p$ are the eigenvalues of ${\hat{\bf M}}$ and $r\leq R\leq p$ is a constant. (In practice take $R=p/2$.)
\item The columns of the estimated factor loading matrix ${\hat{\bf A}}$ are the $\hat{r}$th orthonormal eigenvectors of $\hat{\bf M}$ corresponding to its $\hat{r}$ largest eigenvalues.
\item Estimated factor process and resulting residuals are
${\hat{\bf x}}_t={\hat{\bf A}}^{\sf T}{\bf y}_t,~~{\hat{\bm\epsilon}_t=({\bf I}_p-{\hat{\bf A}}{\hat{\bf A}}^{\sf T}){\hat{\bf y}}_t.$
\item Dynamic modelling of ${\hat{\bf y}}_t$ is ${\hat{\bf y}}_t={\hat{\bf A}}{\hat{\bf x}}_t$.
\end{enumerate}
\section{Numeric Studies}
\subsection{Clustering quality evaluation criteria}

Evaluating clustering systems is not a trivial task because clustering is an unsupervised learning process in the absence of the information of the actual partitions. We used classified datasets and compared how good the clustered results fit with the data labels. As is in \citet{wt_05}, five objective clustering evaluation criteria were used in all of the numerical experiments: Jaccard score, Rand index, Folkes and Mallow index \citep{cv_01}, clustering similarity measure \citep{csm_00} and normalized mutual information \citep{nmi_02}.\\

Consider ${\cal G}=\{G_1,G_2,\ldots,G_M\}$ as the clusters from a supervised dataset and ${\cal A}=\{A_1,A_2,\ldots,A_M\}$ as that obtained by a clustering algorithm under evaluations. Denote $D$ as a dataset of original time series. For all pairs of the series, count the following quantities:
\begin{itemize}
\item[a:] the number of pairs belonging to the same cluster in both $\cal G$ and $\cal A$;
\item[b:] the number of pairs belonging to the same cluster in $\cal G$ but not in $\cal A$;
\item[c:] the number of pairs belonging to the same cluster in $\cal A$ but not in $\cal G$;
\item[d:] the number of pairs belonging to different clusters in both $\cal G$ and $\cal A$. 
\end{itemize} 
The used clustering evaluation criteria are defined as follows 
\begin{itemize}
\item[1] Jacard score
\[{\sf Jacard}({\cal G},{\cal A})=\frac{a}{a+b+c};\]
\item[2] Rand index
\[{\sf Rand}({\cal G},{\cal A})=\frac{a+d}{a+b+c+d};\]
\item[3] Folkes and Mallow index
\[{\sf FMI}({\cal G},{\cal A})=\sqrt{\frac{a}{a+b}\cdot\frac{a}{a+c}};\]
\item[4] Cluster similarity measure
\[{\sf CSM}({\cal G},{\cal A})=\frac{1}{M}\sum_{i=1}^M\max_{1\leq j\leq M}{\sf Sim}(G_i,A_j),~\mbox{where}~{\sf Sim}(G_i,A_j)=\frac{2|G_i\cap A_j|}{|G_i|+|A_j|};\]
\item[5] Normalized Mutual Information
\[{\sf NMI}({\cal G},{\cal A})=\frac{\sum_{i=1}^M\sum_{j=1}^MN_{i,j}\log\left(\frac{N\cdot N_{i,j}}{|G_i|\cdot|A_j|}\right)}{\left(\sum_{i=1}^M|G_i|\log\left(\frac{|G_i|}{N}\right)\right)^{1/2}\cdot\left(\sum_{j=1}^M|A_j|\log\left(\frac{|A_j|}{N}\right)\right)^{1/2}}\]
where $N$ is the total number of time series in the dataset and $N_{i,j}=|G_i\cap A_j|$.
\end{itemize}
All the used clustering evaluation criteria have value ranging from 0 to 1, where 1 corresponds to the case when $\cal G$ and $\cal A$ are identical. The larger the values of criteria, the more similar between $\cal A$ and $\cal G$.
\subsection{Simulation Experiment}
For each replication of the experiment, the dataset $S$ subjected to clustering was formed by one partial realization of length $T=200$ simulated from each of the autoregressive models enumerated in Table \ref{ARM}. At each trial of the experiment, one series of length $T=200$ were generated from each of models M1 to M6. These models were considered in \citep{nf_10} and by other authors in previous works. The dimension of the factor process is six. The algorithm of \citet{KM_79} is used in the implementation of $k-$means clustering. Since the clustering results of $k-$means clustering depend on the initial clustering centers that shoue be randomly initialized in each run, we run $k-$means 100 times with random initialized centres every time. For each iteration, the number of random start for $k-$means algorithm is one. For AR($\infty$) in this part of experiment, the maximum order of model to fit is set as the minimum of $T-1$ and $10\times{\sf round}(\log_{10} T)$ and the estimation method is Yule-Walker. 
\begin{center}
\begin{tabular}{ccc}
\hline
M1 & AR & $X_t=0.6X_{t-1}+\epsilon_t$ \\
M2 & Bilinear & $X_t=(0.3-0.2\epsilon_{t-1})\cdot X_{t-1}+1+\epsilon_t$ \\
M3 & EXPAR & $X_t=(0.9\exp\{-X_{t-1}^2\}-0.6)X_{t-1}+1+\epsilon_t$ \\
M4 & SETAR & $X_t=(0.3X_{t-1}+1)\cdot \mbox{sgn}(X_{t-1}-0.2)+\epsilon_t$ \\
M5 & NLAR & $X_t=0.7|X_{t-1}|\cdot(2+|X_{t-1}|)^{-1}+\epsilon_t$ \\
M6 & STAR & $X_t=0.8X_{t-1}-0.8X_{t-1}\cdot(1+\exp\{-10X_{t-1}\})^{-1}+\epsilon_t$\\
\hline
\end{tabular}
\end{center}
There are four types of dynamics in the loading matrix ${\bf A}$, that is, each row of $\bf A$ equals to one the four types in Table \ref{LM}. There are $k$ rows for each type in the loading matrix $\bf A$. Thus, $p=4\times k$. The number of classes in this simulated dataset is 4 and the ground-truth labels are known.
\begin{center}
\begin{tabular}{cccc}
\hline
Type I & (1,1,1,1,1,1) & Type II & (1,0,0,0,1,0)\\
Type III & (0,1,1,0,0,0) & Type IV & (0,0,0,1,0,1)\\
\hline
\end{tabular}
\end{center}
\input{km_sim.tex}
\subsection{Evaluation of classical datasets}
Ground truth of classification are available in these classical datasets. In this part, the clustering procedure based on factor modelling (FM) is evaluated on XXX datasets. Six classified datasets in UCR Time Series Classification/Clustering Page \citep{UCR}. The reason why these six datasets are chosen is that {\sf .......Explain it!}. 
\begin{center}
[\sf UCR Dataset description!]
\end{center}
Euclidean distance is used for both $k-$means and hierarchical clustering algorithms. The $k-$means clustering was conducted 100 times with random initialized centres for each experiment. As each run of hierarchical clustering for the same dataset always gets the same result, multiple runs are not needed in hierarchical clustering procedures. We use ward linkage \citep{ward} for the hierarchical clustering algorithm in our experiment. Table XXX lists the mean of evaluation criteria values of 100 runs for $k-$means with the estimated factor loading matrix (FM), the original data (OR), the extracted features using wavelet transformation (DW) and the matrix of AR$(\infty)$ coefficients (AP) for each datasets. Table XXX presents the evaluation criteria values produced by these four methods for each datasets. {\sf Methods compared here:}
\begin{itemize} 
\item[DW] An unsupervised feature extraction was performed for time series clustering using orthogonal wavelet transform \citep{wt_05}. The original time series could be well approximated by their wavelet coefficients in some proper dimension. The desired dimension of features is determined by minimizing the dimensionality and sum of squared errors between features and the original time series simultaneously. Time efficiency of this feature extraction method is guaranteed when Haar wavelet is used.
\item[DP] For clustering a set of dynamic structures on the class of autoregressive integrated moving-average invertible models, \citet{dp_90} used the Euclidean distance of their AR($\infty$) operator coefficients to measure the structural diversity between two time series. In the experiments, the order parameter of the autoregressive model is chosen by Akaike Information Criterion and the method used to fit the model is 
\end{itemize}
\input{ucr.tex}
\section{Data Analysis}
[{\sf TBA}]
\section{Discussion}
\begin{itemize}
\item Time series clustering $\longrightarrow$ Ordinary clustering problem; Difficulty for clustering time series can be avoided. {\sf Enrich this argument!}
\item Assume that the dynamic part of univariate time series $\{y_{kt}\}$ for each variate $k$, $k\in[p]$ can be described by latent factor process $\{{\bf x}_t\}$ and its loading vector $A_k$. Find some effective test statistics to test the null hypothesis that there is no difference between the dynamic part of these two series, that is, ${\rm H}_0: A_i=A_j$. Two series are clustered together if the associated $p-$value is greater than the pre-specified significance level. {\sf Try to find such test statistic; even if proposed in the next paper.}
\item Co-clustering: consider jointly clustering. \citep{bc_05}
\end{itemize}
\section{Conclusion}
The general philosophy of this paper can be summarized as follows. The high-dimensional multivariate time series are decompose into two parts: a dynamic part driven by a lower-dimensional factor process and static part which is a vector white noise. In this factor model, the rows of loading matrix convey all the useful information about the stochastic dynamic structure of the original multiple process. Differences between variates in the raw data can be well approximated by those between rows in the loading matrix.
\bibliographystyle{Chicago}
\bibliography{FMclustering}
\end{document}
